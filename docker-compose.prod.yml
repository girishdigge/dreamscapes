services:
  # Production overrides for optimized performance

  # LLaMA FastAPI microservice - production mode
  llama-stylist:
    build:
      context: ./services/llama-stylist
      target: production
    environment:
      - PYTHONPATH=/app
      - PORT=8000
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
    healthcheck:
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Render Worker - production mode
  render-worker:
    build:
      context: ./services/render-worker
      target: production
    environment:
      - NODE_ENV=production
      - PORT=8001
      - PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium
      - CHROMIUM_PATH=/usr/bin/chromium
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    healthcheck:
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # MCP Gateway - production mode with optimized AI provider integration
  mcp-gateway:
    build:
      context: ./services/mcp-gateway
      target: production
    environment:
      - NODE_ENV=production
      - PORT=8080
      # Production Cerebras Configuration
      - CEREBRAS_API_KEY=${CEREBRAS_API_KEY}
      - CEREBRAS_API_URL=${CEREBRAS_API_URL:-https://api.cerebras.ai/v1}
      - CEREBRAS_MODEL=${CEREBRAS_MODEL:-llama-3.3-70b}
      - CEREBRAS_TEMPERATURE=${CEREBRAS_TEMPERATURE:-0.6}
      - CEREBRAS_TOP_P=${CEREBRAS_TOP_P:-0.9}
      - CEREBRAS_MAX_TOKENS=${CEREBRAS_MAX_TOKENS:-32768}
      - CEREBRAS_STREAM=${CEREBRAS_STREAM:-true}
      # Provider Configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4}
      - LLAMA_URL=http://llama-stylist:8000
      - RENDER_WORKER_URL=http://render-worker:8001
      # Production Caching Configuration
      - REDIS_URL=redis://redis:6379
      - CACHE_DEFAULT_TTL=${CACHE_DEFAULT_TTL:-7200}
      - CACHE_MAX_SIZE=${CACHE_MAX_SIZE:-50000}
      - CACHE_ENABLE_SEMANTIC_SIMILARITY=${CACHE_ENABLE_SEMANTIC_SIMILARITY:-true}
      - CACHE_QUALITY_THRESHOLD=${CACHE_QUALITY_THRESHOLD:-0.85}
      # Production Provider Management
      - PROVIDER_FALLBACK_ENABLED=${PROVIDER_FALLBACK_ENABLED:-true}
      - PROVIDER_RETRY_ATTEMPTS=${PROVIDER_RETRY_ATTEMPTS:-5}
      - PROVIDER_RETRY_DELAY=${PROVIDER_RETRY_DELAY:-2000}
      - PROVIDER_TIMEOUT=${PROVIDER_TIMEOUT:-45000}
      - PROVIDER_CIRCUIT_BREAKER_THRESHOLD=${PROVIDER_CIRCUIT_BREAKER_THRESHOLD:-3}
      # Production Monitoring
      - ENABLE_METRICS=${ENABLE_METRICS:-true}
      - METRICS_INTERVAL=${METRICS_INTERVAL:-120000}
      - LOG_LEVEL=${LOG_LEVEL:-warn}
    depends_on:
      llama-stylist:
        condition: service_healthy
      render-worker:
        condition: service_started
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Express orchestrator - production mode
  express:
    build:
      context: ./services/express
      target: production
    environment:
      - NODE_ENV=production
      - PORT=8000
      - MCP_GATEWAY_URL=http://mcp-gateway:8080
      - LLAMA_URL=http://llama-stylist:8000
      - RENDER_WORKER_URL=http://render-worker:8001
      - ENABLE_FALLBACK_DEMOS=${ENABLE_FALLBACK_DEMOS:-false}
      - MAX_CACHE_SIZE=${MAX_CACHE_SIZE:-5000}
      - DEFAULT_RENDER_QUALITY=${DEFAULT_RENDER_QUALITY:-high}
      - CACHE_TTL=${CACHE_TTL:-7200000}
    depends_on:
      mcp-gateway:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'
    healthcheck:
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Frontend - production mode with optimized build
  frontend:
    build:
      context: ./services/frontend/next-app
      target: production
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - PORT=3000
    depends_on:
      express:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'
    healthcheck:
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Redis for production caching with enhanced AI provider optimization
  redis:
    image: redis:7-alpine
    ports:
      - '6379:6379'
    command: redis-server --appendonly yes --maxmemory ${REDIS_MAX_MEMORY:-1gb} --maxmemory-policy allkeys-lru --save 300 100 --tcp-keepalive 60
    volumes:
      - redis_data:/data
    environment:
      - REDIS_REPLICATION_MODE=master
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

volumes:
  redis_data:

networks:
  default:
    driver: bridge
