services:
  # LLaMA FastAPI microservice - DISABLED (not needed for MVP)
  # llama-stylist:
  #   build:
  #     context: ./services/llama-stylist
  #     target: development
  #   ports:
  #     - '8002:8000'
  #   environment:
  #     - PYTHONPATH=/app
  #     - PORT=8000
  #   healthcheck:
  #     test: ['CMD', 'curl', '-f', 'http://localhost:8000/health']
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   restart: unless-stopped

  # Render Worker - Puppeteer + FFmpeg for video generation
  render-worker:
    build:
      context: ./services/render-worker
      target: development # Default to development, override in prod
    ports:
      - '8001:8001'
    environment:
      - NODE_ENV=${NODE_ENV:-development}
      - PORT=8001
      - PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium
      - CHROMIUM_PATH=/usr/bin/chromium
    healthcheck:
      test:
        [
          'CMD',
          'node',
          '-e',
          'require("http").get("http://localhost:8001/health", (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on("error", () => process.exit(1))',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # MCP Gateway - Enhanced AI provider proxy with Cerebras SDK
  mcp-gateway:
    build:
      context: .
      dockerfile: ./services/mcp-gateway/Dockerfile
      target: development # Default to development, override in prod
    volumes:
      - ./shared:/app/shared:ro
    ports:
      - '8080:8080'
    environment:
      - NODE_ENV=${NODE_ENV:-development}
      - PORT=8080
      # Enhanced Cerebras Configuration
      - CEREBRAS_API_KEY=${CEREBRAS_API_KEY}
      - CEREBRAS_API_URL=${CEREBRAS_API_URL:-https://api.cerebras.ai/v1}
      - CEREBRAS_MODEL=${CEREBRAS_MODEL:-llama-3.3-70b}
      - CEREBRAS_TEMPERATURE=${CEREBRAS_TEMPERATURE:-0.6}
      - CEREBRAS_TOP_P=${CEREBRAS_TOP_P:-0.9}
      - CEREBRAS_MAX_TOKENS=${CEREBRAS_MAX_TOKENS:-32768}
      - CEREBRAS_STREAM=${CEREBRAS_STREAM:-true}
      # Provider Configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4}
      # - LLAMA_URL=http://llama-stylist:8000  # Disabled - not needed for MVP
      - RENDER_WORKER_URL=http://render-worker:8001
      # Enhanced Caching Configuration
      - REDIS_URL=redis://redis:6379
      - CACHE_DEFAULT_TTL=${CACHE_DEFAULT_TTL:-3600}
      - CACHE_MAX_SIZE=${CACHE_MAX_SIZE:-10000}
      - CACHE_ENABLE_SEMANTIC_SIMILARITY=${CACHE_ENABLE_SEMANTIC_SIMILARITY:-true}
      - CACHE_QUALITY_THRESHOLD=${CACHE_QUALITY_THRESHOLD:-0.8}
      # Provider Management
      - PROVIDER_FALLBACK_ENABLED=${PROVIDER_FALLBACK_ENABLED:-true}
      - PROVIDER_RETRY_ATTEMPTS=${PROVIDER_RETRY_ATTEMPTS:-3}
      - PROVIDER_RETRY_DELAY=${PROVIDER_RETRY_DELAY:-1000}
      - PROVIDER_TIMEOUT=${PROVIDER_TIMEOUT:-30000}
      - PROVIDER_CIRCUIT_BREAKER_THRESHOLD=${PROVIDER_CIRCUIT_BREAKER_THRESHOLD:-5}
      # Monitoring and Metrics
      - ENABLE_METRICS=${ENABLE_METRICS:-true}
      - METRICS_INTERVAL=${METRICS_INTERVAL:-60000}
      - LOG_LEVEL=${LOG_LEVEL:-info}
    depends_on:
      # llama-stylist:  # Disabled - not needed for MVP
      #   condition: service_healthy
      render-worker:
        condition: service_started
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test:
        [
          'CMD',
          'node',
          '-e',
          'require("http").get("http://localhost:8080/health", (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on("error", () => process.exit(1))',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # Express orchestrator - main API
  express:
    build:
      context: .
      dockerfile: ./services/express/Dockerfile
      target: development # Default to development, override in prod
    volumes:
      - ./shared:/app/shared:ro
      - ./sample_dreams:/app/sample_dreams:ro
    ports:
      - '8000:8000'
    environment:
      - NODE_ENV=${NODE_ENV:-development}
      - PORT=8000
      - MCP_GATEWAY_URL=http://mcp-gateway:8080
      # - LLAMA_URL=http://llama-stylist:8000  # Disabled - not needed for MVP
      - RENDER_WORKER_URL=http://render-worker:8001
      - ENABLE_FALLBACK_DEMOS=${ENABLE_FALLBACK_DEMOS:-true}
      - MAX_CACHE_SIZE=${MAX_CACHE_SIZE:-1000}
      - DEFAULT_RENDER_QUALITY=${DEFAULT_RENDER_QUALITY:-draft}
      - CACHE_TTL=${CACHE_TTL:-3600000}
    depends_on:
      mcp-gateway:
        condition: service_healthy
    healthcheck:
      test:
        [
          'CMD',
          'node',
          '-e',
          'require("http").get("http://localhost:8000/health", (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on("error", () => process.exit(1))',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # Frontend - Next.js React app
  frontend:
    build:
      context: ./services/frontend/next-app
      target: development # Default to development, override in prod
    volumes:
      # Mount sample_dreams directory for API access
      - ./sample_dreams:/app/sample_dreams:ro
    ports:
      - '3000:3000'
    environment:
      - NODE_ENV=${NODE_ENV:-development}
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - API_URL=http://express:8000
      - PORT=3000
    depends_on:
      express:
        condition: service_healthy
    healthcheck:
      test:
        [
          'CMD',
          'node',
          '-e',
          'require("http").get("http://localhost:3000", (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on("error", () => process.exit(1))',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Redis for enhanced caching system with AI provider optimization
  redis:
    image: redis:7-alpine
    ports:
      - '6379:6379'
    command: redis-server --appendonly yes --maxmemory ${REDIS_MAX_MEMORY:-512mb} --maxmemory-policy allkeys-lru --save 60 1000
    volumes:
      - redis_data:/data
    environment:
      - REDIS_REPLICATION_MODE=master
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

# Named volumes
volumes:
  redis_data:

networks:
  default:
    driver: bridge
